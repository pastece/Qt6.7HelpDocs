<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
<!-- qttest-best-practices.qdoc -->
  <meta name="description" content="Guidelines for creating Qt tests.">
  <title>Qt Test Best Practices | Qt Test 6.7.0</title>
  <link rel="stylesheet" type="text/css" href="style/offline-simple.css" />
  <script type="text/javascript">
    document.getElementsByTagName("link").item(0).setAttribute("href", "style/offline.css");
    // loading style sheet breaks anchors that were jumped to before
    // so force jumping to anchor again
    setTimeout(function() {
        var anchor = location.hash;
        // need to jump to different anchor first (e.g. none)
        location.hash = "#";
        setTimeout(function() {
            location.hash = anchor;
        }, 0);
    }, 0);
  </script>
</head>
<body>
<div class="header" id="qtdocheader">
    <div class="main">
    <div class="main-rounded">
        <div class="navigationbar">
        <ul>
<li><a href="../qtdoc/index.html" translate="no">Qt 6.7</a></li>
<li><a href="qttest-index.html" translate="no">Qt Test</a></li>
<li>Qt Test Best Practices</li>
<li id="buildversion"><a href="qttest-index.html" translate="no">Qt 6.7.0 Reference Documentation</a></li>
    </ul>
    </div>
</div>
<div class="content">
<div class="line">
<div class="content mainContent">
<div class="sidebar">
<div class="toc">
<h3 id="toc">Contents</h3>
<ul>
<li class="level1"><a href="#general-principles">General Principles</a></li>
<li class="level2"><a href="#verify-tests">Verify Tests</a></li>
<li class="level2"><a href="#give-test-functions-descriptive-names">Give Test Functions Descriptive Names</a></li>
<li class="level2"><a href="#write-self-contained-test-functions">Write Self-contained Test Functions</a></li>
<li class="level2"><a href="#test-the-full-stack">Test the Full Stack</a></li>
<li class="level2"><a href="#make-tests-complete-quickly">Make Tests Complete Quickly</a></li>
<li class="level2"><a href="#use-data-driven-testing">Use Data-driven Testing</a></li>
<li class="level2"><a href="#use-coverage-tools">Use Coverage Tools</a></li>
<li class="level2"><a href="#select-appropriate-mechanisms-to-exclude-tests">Select Appropriate Mechanisms to Exclude Tests</a></li>
<li class="level2"><a href="#avoid-q-assert">Avoid Q_ASSERT</a></li>
<li class="level1"><a href="#writing-reliable-tests">Writing Reliable Tests</a></li>
<li class="level2"><a href="#avoid-side-effects-in-verification-steps">Avoid Side-effects in Verification Steps</a></li>
<li class="level2"><a href="#avoid-fixed-timeouts">Avoid Fixed Timeouts</a></li>
<li class="level2"><a href="#beware-of-timing-dependent-behavior">Beware of Timing-dependent Behavior</a></li>
<li class="level2"><a href="#avoid-bitmap-capture-and-comparison">Avoid Bitmap Capture and Comparison</a></li>
<li class="level1"><a href="#improving-test-output">Improving Test Output</a></li>
<li class="level2"><a href="#test-for-warnings">Test for Warnings</a></li>
<li class="level2"><a href="#avoid-printing-debug-messages-from-autotests">Avoid Printing Debug Messages from Autotests</a></li>
<li class="level2"><a href="#write-well-structured-diagnostic-code">Write Well-structured Diagnostic Code</a></li>
<li class="level1"><a href="#writing-testable-code">Writing Testable Code</a></li>
<li class="level2"><a href="#break-dependencies">Break Dependencies</a></li>
<li class="level2"><a href="#compile-all-classes-into-libraries">Compile All Classes into Libraries</a></li>
<li class="level1"><a href="#setting-up-test-machines">Setting up Test Machines</a></li>
<li class="level2"><a href="#screen-savers">Screen Savers</a></li>
<li class="level2"><a href="#system-dialogs">System Dialogs</a></li>
<li class="level2"><a href="#display-usage">Display Usage</a></li>
<li class="level2"><a href="#window-managers">Window Managers</a></li>
</ul>
</div>
<div class="sidebar-content" id="sidebar-content"></div></div>
<h1 class="title">Qt Test Best Practices</h1>
<!-- $$$qttest-best-practices.qdoc-description -->
<div class="descr" id="details">
<p>We recommend that you add Qt tests for bug fixes and new features. Before you try to fix a bug, add a <i>regression test</i> (ideally automatic) that fails before the fix, exhibiting the bug, and passes after the fix. While you're developing new features, add tests to verify that they work as intended.</p>
<p>Conforming to a set of coding standards will make it more likely for Qt autotests to work reliably in all environments. For example, some tests need to read data from disk. If no standards are set for how this is done, some tests won't be portable. For example, a test that assumes its test-data files are in the current working directory only works for an in-source build. In a shadow build (outside the source directory), the test will fail to find its data.</p>
<p>The following sections contain guidelines for writing Qt tests:</p>
<ul>
<li><a href="qttest-best-practices-qdoc.html#general-principles" translate="no">General Principles</a></li>
<li><a href="qttest-best-practices-qdoc.html#writing-reliable-tests" translate="no">Writing Reliable Tests</a></li>
<li><a href="qttest-best-practices-qdoc.html#improving-test-output" translate="no">Improving Test Output</a></li>
<li><a href="qttest-best-practices-qdoc.html#writing-testable-code" translate="no">Writing Testable Code</a></li>
<li><a href="qttest-best-practices-qdoc.html#setting-up-test-machines" translate="no">Setting up Test Machines</a></li>
</ul>
<h2 id="general-principles">General Principles</h2>
<p>The following sections provide general guidelines for writing unit tests:</p>
<ul>
<li><a href="qttest-best-practices-qdoc.html#verify-tests" translate="no">Verify Tests</a></li>
<li><a href="qttest-best-practices-qdoc.html#give-test-functions-descriptive-names" translate="no">Give Test Functions Descriptive Names</a></li>
<li><a href="qttest-best-practices-qdoc.html#write-self-contained-test-functions" translate="no">Write Self-contained Test Functions</a></li>
<li><a href="qttest-best-practices-qdoc.html#test-the-full-stack" translate="no">Test the Full Stack</a></li>
<li><a href="qttest-best-practices-qdoc.html#make-tests-complete-quickly" translate="no">Make Tests Complete Quickly</a></li>
<li><a href="qttest-best-practices-qdoc.html#use-data-driven-testing" translate="no">Use Data-driven Testing</a></li>
<li><a href="qttest-best-practices-qdoc.html#use-coverage-tools" translate="no">Use Coverage Tools</a></li>
<li><a href="qttest-best-practices-qdoc.html#select-appropriate-mechanisms-to-exclude-tests" translate="no">Select Appropriate Mechanisms to Exclude Tests</a></li>
<li><a href="qttest-best-practices-qdoc.html#avoid-q-assert" translate="no">Avoid Q_ASSERT</a></li>
</ul>
<h3 id="verify-tests">Verify Tests</h3>
<p>Write and commit your tests along with your fix or new feature on a new branch. Once you're done, you can check out the branch on which your work is based, and then check out into this branch the test-files for your new tests. This enables you to verify that the tests do fail on the prior branch, and therefore actually do catch a bug or test a new feature.</p>
<p>For example, the workflow to fix a bug in the <code translate="no">QDateTime</code> class could be like this if you use the Git version control system:</p>
<ol class="1" type="1"><li>Create a branch for your fix and test: <code translate="no">git checkout -b fix-branch 5.14</code></li>
<li>Write a test and fix the bug.</li>
<li>Build and test with both the fix and the new test, to verify that the new test passes with the fix.</li>
<li>Add the fix and test to your branch: <code translate="no">git add tests/auto/corelib/time/qdatetime/tst_qdatetime.cpp src/corelib/time/qdatetime.cpp</code></li>
<li>Commit the fix and test to your branch: <code translate="no">git commit -m 'Fix bug in QDateTime'</code></li>
<li>To verify that the test actually catches something for which you needed the fix, checkout the branch you based your own branch on: <code translate="no">git checkout 5.14</code></li>
<li>Checkout only the test file to the 5.14 branch: <code translate="no">git checkout fix-branch -- tests/auto/corelib/time/qdatetime/tst_qdatetime.cpp</code><p>Only the test is now on the fix-branch. The rest of the source tree is still on 5.14.</p>
</li>
<li>Build and run the test to verify that it fails on 5.14, and therefore does indeed catch a bug.</li>
<li>You can now return to the fix branch: <code translate="no">git checkout fix-branch</code></li>
<li>Alternatively, you can restore your work tree to a clean state on 5.14: <code translate="no">git checkout HEAD -- tests/auto/corelib/time/qdatetime/tst_qdatetime.cpp</code></li>
</ol>
<p>When you're reviewing a change, you can adapt this workflow to check that the change does indeed come with a test for a problem it does fix.</p>
<h3 id="give-test-functions-descriptive-names">Give Test Functions Descriptive Names</h3>
<p>Naming test cases is important. The test name appears in the failure report for a test run. For data-driven tests, the name of the data row also appears in the failure report. The names give those reading the report a first indication of what has gone wrong.</p>
<p>Test function names should make it obvious what the function is trying to test. Do not simply use the bug-tracking identifier, because the identifiers become obsolete if the bug-tracker is replaced. Also, some bug-trackers may not be accessible to all users. When the bug report may be of interest to later readers of the test code, you can mention it in a comment alongside a relevant part of the test.</p>
<p>Likewise, when writing data-driven tests, give descriptive names to the test-cases, that indicate what aspect of the functionality each focuses on. Do not simply number the test-case, or use bug-tracking identifiers. Someone reading the test output will have no idea what the numbers or identifiers mean. You can add a comment on the test-row that mentions the bug-tracking identifier, when relevant. It's best to avoid spacing characters and characters that may be significant to command-line shells on which you may want to run tests. This makes it easier to specify the test and tag on <a href="qtest-overview.html#qt-test-command-line-arguments" translate="no">the command-line</a> to your test program - for example, to limit a test run to just one test-case.</p>
<h3 id="write-self-contained-test-functions">Write Self-contained Test Functions</h3>
<p>Within a test program, test functions should be independent of each other and they should not rely upon previous test functions having been run. You can check this by running the test function on its own with <code translate="no">tst_foo testname</code>.</p>
<p>Do not re-use instances of the class under test in several tests. Test instances (for example widgets) should not be member variables of the tests, but preferably be instantiated on the stack to ensure proper cleanup even if a test fails, so that tests do not interfere with each other.</p>
<h3 id="test-the-full-stack">Test the Full Stack</h3>
<p>If an API is implemented in terms of pluggable or platform-specific backends that do the heavy-lifting, make sure to write tests that cover the code-paths all the way down into the backends. Testing the upper layer API parts using a mock backend is a nice way to isolate errors in the API layer from the backends, but it is complementary to tests that run the actual implementation with real-world data.</p>
<h3 id="make-tests-complete-quickly">Make Tests Complete Quickly</h3>
<p>Tests should not waste time by being unnecessarily repetitious, by using inappropriately large volumes of test data, or by introducing needless idle time.</p>
<p>This is particularly true for unit testing, where every second of extra unit test execution time makes CI testing of a branch across multiple targets take longer. Remember that unit testing is separate from load and reliability testing, where larger volumes of test data and longer test runs are expected.</p>
<p>Benchmark tests, which typically execute the same test multiple times, should be located in a separate <code translate="no">tests/benchmarks</code> directory and they should not be mixed with functional unit tests.</p>
<h3 id="use-data-driven-testing">Use Data-driven Testing</h3>
<p><a href="qttestlib-tutorial2-example.html" translate="no">Data-driven tests</a> make it easier to add new tests for boundary conditions found in later bug reports.</p>
<p>Using a data-driven test rather than testing several items in sequence in a test saves repetition of very similar code and ensures later cases are tested even when earlier ones fail. It also encourages systematic and uniform testing, because the same tests are applied to each data sample.</p>
<p>When a test is data-driven, you can specify its data-tag along with the test-function name, as <code translate="no">function:tag</code>, on the command-line of the test to run the test on just one specific test-case, rather than all test-cases of the function. This can be used for either a global data tag or a local tag, identifying a row from the function's own data; you can even combine them as <code translate="no">function:global:local</code>.</p>
<h3 id="use-coverage-tools">Use Coverage Tools</h3>
<p>Use a coverage tool such as <a href="https://www.qt.io/product/quality-assurance/coco" translate="no">Coco</a> or <a href="https://gcc.gnu.org/onlinedocs/gcc/Gcov.html" translate="no">gcov</a> to help write tests that cover as many statements, branches, and conditions as possible in the function or class being tested. The earlier this is done in the development cycle for a new feature, the easier it will be to catch regressions later when the code is refactored.</p>
<h3 id="select-appropriate-mechanisms-to-exclude-tests">Select Appropriate Mechanisms to Exclude Tests</h3>
<p>It is important to select the appropriate mechanism to exclude inapplicable tests.</p>
<p>Use <a href="qtest.html#QSKIP" translate="no">QSKIP</a>() to handle cases where a whole test function is found at run-time to be inapplicable in the current test environment. When just a part of a test function is to be skipped, a conditional statement can be used, optionally with a <code translate="no">qDebug()</code> call to report the reason for skipping the inapplicable part.</p>
<p>When there are known test failures that should eventually be fixed, <a href="qtest.html#QEXPECT_FAIL" translate="no">QEXPECT_FAIL</a> is recommended, as it supports running the rest of the test, when possible. It also verifies that the issue still exists, and lets the code's maintainer know if they unwittingly fix it, a benefit which is gained even when using the <a href="qtest.html#TestFailMode-enum" translate="no">Abort</a> flag.</p>
<p>Test functions or data rows of a data-driven test can be limited to particular platforms, or to particular features being enabled using <code translate="no">#if</code>. However, beware of <a href="../qtdoc/moc.html" translate="no">moc</a> limitations when using <code translate="no">#if</code> to skip test functions. The <code translate="no">moc</code> preprocessor does not have access to all the <code translate="no">builtin</code> macros of the compiler that are often used for feature detection of the compiler. Therefore, <code translate="no">moc</code> might get a different result for a preprocessor condition from that seen by the rest of your code. This may result in <code translate="no">moc</code> generating meta-data for a test slot that the actual compiler skips, or omitting the meta-data for a test slot that is actually compiled into the class. In the first case, the test will attempt to run a slot that is not implemented. In the second case, the test will not attempt to run a test slot even though it should.</p>
<p>If an entire test program is inapplicable for a specific platform or unless a particular feature is enabled, the best approach is to use the parent directory's build configuration to avoid building the test. For example, if the <code translate="no">tests/auto/gui/someclass</code> test is not valid for macOS, wrap its inclusion as a subdirectory in <code translate="no">tests/auto/gui/CMakeLists.txt</code> in a platform check:</p>
<pre class="cpp plain" translate="no">
 if(NOT APPLE)
     add_subdirectory(someclass)
 endif
</pre>
<p>or, if using <code translate="no">qmake</code>, add the following line to <code translate="no">tests/auto/gui.pro</code>:</p>
<pre class="cpp plain" translate="no">
 mac*: SUBDIRS -= someclass
</pre>
<p>See also <a href="qttestlib-tutorial6.html" translate="no">Skipping Tests with QSKIP</a>.</p>
<h3 id="avoid-q-assert">Avoid Q_ASSERT</h3>
<p>The <a href="../qtcore/qtassert-qtcore-proxy.html#Q_ASSERT" translate="no">Q_ASSERT</a> macro causes a program to abort whenever the asserted condition is <code translate="no">false</code>, but only if the software was built in debug mode. In both release and debug-and-release builds, <code translate="no">Q_ASSERT</code> does nothing.</p>
<p><code translate="no">Q_ASSERT</code> should be avoided because it makes tests behave differently depending on whether a debug build is being tested, and because it causes a test to abort immediately, skipping all remaining test functions and returning incomplete or malformed test results.</p>
<p>It also skips any tear-down or tidy-up that was supposed to happen at the end of the test, and might therefore leave the workspace in an untidy state, which might cause complications for further tests.</p>
<p>Instead of <code translate="no">Q_ASSERT</code>, the <a href="qtest.html#QCOMPARE" translate="no">QCOMPARE</a>() or <a href="qtest.html#QVERIFY" translate="no">QVERIFY</a>() macro variants should be used. They cause the current test to report a failure and terminate, but allow the remaining test functions to be executed and the entire test program to terminate normally. <a href="qtest.html#QVERIFY2" translate="no">QVERIFY2</a>() even allows a descriptive error message to be recorded in the test log.</p>
<h2 id="writing-reliable-tests">Writing Reliable Tests</h2>
<p>The following sections provide guidelines for writing reliable tests:</p>
<ul>
<li><a href="qttest-best-practices-qdoc.html#avoid-side-effects-in-verification-steps" translate="no">Avoid Side-effects in Verification Steps</a></li>
<li><a href="qttest-best-practices-qdoc.html#avoid-fixed-timeouts" translate="no">Avoid Fixed Timeouts</a></li>
<li><a href="qttest-best-practices-qdoc.html#beware-of-timing-dependent-behavior" translate="no">Beware of Timing-dependent Behavior</a></li>
<li><a href="qttest-best-practices-qdoc.html#avoid-bitmap-capture-and-comparison" translate="no">Avoid Bitmap Capture and Comparison</a></li>
</ul>
<h3 id="avoid-side-effects-in-verification-steps">Avoid Side-effects in Verification Steps</h3>
<p>When performing verification steps in an autotest using <a href="qtest.html#QCOMPARE" translate="no">QCOMPARE</a>(), <a href="qtest.html#QVERIFY" translate="no">QVERIFY</a>(), and so on, side-effects should be avoided. Side-effects in verification steps can make a test difficult to understand. Also, they can easily break a test in ways that are difficult to diagnose when the test is changed to use <a href="qtest.html#QTRY_VERIFY" translate="no">QTRY_VERIFY</a>(), <a href="qtest.html#QTRY_COMPARE" translate="no">QTRY_COMPARE</a>() or <a href="qtest.html#QBENCHMARK" translate="no">QBENCHMARK</a>(). These can execute the passed expression multiple times, thus repeating any side-effects.</p>
<p>When side-effects are unavoidable, ensure that the prior state is restored at the end of the test function, even if the test fails. This commonly requires use of an RAII (resource acquisition is initialization) class that restores state when the function returns, or a <code translate="no">cleanup()</code> method. Do not simply put the restoration code at the end of the test. If part of the test fails, such code will be skipped and the prior state will not be restored.</p>
<h3 id="avoid-fixed-timeouts">Avoid Fixed Timeouts</h3>
<p>Avoid using hard-coded timeouts, such as <a href="qtest.html#qWait" translate="no">QTest::qWait</a>() to wait for some conditions to become true. Consider using the <a href="qsignalspy.html" translate="no">QSignalSpy</a> class, the <a href="qtest.html#QTRY_VERIFY" translate="no">QTRY_VERIFY</a>() or <a href="qtest.html#QTRY_COMPARE" translate="no">QTRY_COMPARE</a>() macros, or the <code translate="no">QSignalSpy</code> class in conjunction with the <code translate="no">QTRY_</code> macro variants.</p>
<p>The <code translate="no">qWait()</code> function can be used to set a delay for a fixed period between performing some action and waiting for some asynchronous behavior triggered by that action to be completed. For example, changing the state of a widget and then waiting for the widget to be repainted. However, such timeouts often cause failures when a test written on a workstation is executed on a device, where the expected behavior might take longer to complete. Increasing the fixed timeout to a value several times larger than needed on the slowest test platform is not a good solution, because it slows down the test run on all platforms, particularly for table-driven tests.</p>
<p>If the code under test issues Qt signals on completion of the asynchronous behavior, a better approach is to use the <a href="qsignalspy.html" translate="no">QSignalSpy</a> class to notify the test function that the verification step can now be performed.</p>
<p>If there are no Qt signals, use the <code translate="no">QTRY_COMPARE()</code> and <code translate="no">QTRY_VERIFY()</code> macros, which periodically test a specified condition until it becomes true or some maximum timeout is reached. These macros prevent the test from taking longer than necessary, while avoiding breakages when tests are written on workstations and later executed on embedded platforms.</p>
<p>If there are no Qt signals, and you are writing the test as part of developing a new API, consider whether the API could benefit from the addition of a signal that reports the completion of the asynchronous behavior.</p>
<h3 id="beware-of-timing-dependent-behavior">Beware of Timing-dependent Behavior</h3>
<p>Some test strategies are vulnerable to timing-dependent behavior of certain classes, which can lead to tests that fail only on certain platforms or that do not return consistent results.</p>
<p>One example of this is text-entry widgets, which often have a blinking cursor that can make comparisons of captured bitmaps succeed or fail depending on the state of the cursor when the bitmap is captured. This, in turn, may depend on the speed of the machine executing the test.</p>
<p>When testing classes that change their state based on timer events, the timer-based behavior needs to be taken into account when performing verification steps. Due to the variety of timing-dependent behavior, there is no single generic solution to this testing problem.</p>
<p>For text-entry widgets, potential solutions include disabling the cursor blinking behavior (if the API provides that feature), waiting for the cursor to be in a known state before capturing a bitmap (for example, by subscribing to an appropriate signal if the API provides one), or excluding the area containing the cursor from the bitmap comparison.</p>
<h3 id="avoid-bitmap-capture-and-comparison">Avoid Bitmap Capture and Comparison</h3>
<p>While verifying test results by capturing and comparing bitmaps is sometimes necessary, it can be quite fragile and labor-intensive.</p>
<p>For example, a particular widget may have different appearance on different platforms or with different widget styles, so reference bitmaps may need to be created multiple times and then maintained in the future as Qt's set of supported platforms evolves. Making changes that affect the bitmap thus means having to recreate the expected bitmaps on each supported platform, which would require access to each platform.</p>
<p>Bitmap comparisons can also be influenced by factors such as the test machine's screen resolution, bit depth, active theme, color scheme, widget style, active locale (currency symbols, text direction, and so on), font size, transparency effects, and choice of window manager.</p>
<p>Where possible, use programmatic means, such as verifying properties of objects and variables, instead of capturing and comparing bitmaps.</p>
<h2 id="improving-test-output">Improving Test Output</h2>
<p>The following sections provide guidelines for producing readable and helpful test output:</p>
<ul>
<li><a href="qttest-best-practices-qdoc.html#test-for-warnings" translate="no">Test for Warnings</a></li>
<li><a href="qttest-best-practices-qdoc.html#avoid-printing-debug-messages-from-autotests" translate="no">Avoid Printing Debug Messages from Autotests</a></li>
<li><a href="qttest-best-practices-qdoc.html#write-well-structured-diagnostic-code" translate="no">Write Well-structured Diagnostic Code</a></li>
</ul>
<h3 id="test-for-warnings">Test for Warnings</h3>
<p>Just as when building your software, if test output is cluttered with warnings you will find it harder to notice a warning that really is a clue to the emergence of a bug. It is thus prudent to regularly check your test logs for warnings, and other extraneous output, and investigate the causes. When they are signs of a bug, you can make warnings trigger test failure.</p>
<p>When the code under test <i>should</i> produce messages, such as warnings about misguided use, it is also important to test that it <i>does</i> produce them when so used. You can test for expected messages from the code under test, produced by <a href="../qtcore/qtlogging.html#qWarning" translate="no">qWarning</a>(), <a href="../qtcore/qtlogging.html#qDebug" translate="no">qDebug</a>(), <a href="../qtcore/qtlogging.html#qInfo" translate="no">qInfo</a>() and friends, using <a href="qtest.html#ignoreMessage" translate="no">QTest::ignoreMessage</a>(). This will verify that the message is produced and filter it out of the output of the test run. If the message is not produced, the test will fail.</p>
<p>If an expected message is only output when Qt is built in debug mode, use <a href="../qtcore/qlibraryinfo.html#isDebugBuild" translate="no">QLibraryInfo::isDebugBuild</a>() to determine whether the Qt libraries were built in debug mode. Using <code translate="no">#ifdef QT_DEBUG</code> is not enough, as it will only tell you whether <i>the test</i> was built in debug mode, and that does not guarantee that the <i>Qt libraries</i> were also built in debug mode.</p>
<p>Your tests can (since Qt 6.3) verify that they do not trigger calls to <a href="../qtcore/qtlogging.html#qWarning" translate="no">qWarning</a>() by calling <a href="qtest.html#failOnWarning" translate="no">QTest::failOnWarning</a>(). This takes the warning message to test for or a <a href="../qtcore/qregularexpression.html" translate="no">QRegularExpression</a> to match against warnings; if a matching warning is produced, it will be reported and cause the test to fail. For example, a test that should produce no warnings at all can <code translate="no">QTest::failOnWarning(QRegularExpression(u&quot;.*&quot;_s))</code>, which will match any warning at all.</p>
<p>You can also set the environment variable <code translate="no">QT_FATAL_WARNINGS</code> to cause warnings to be treated as fatal errors. See <a href="../qtcore/qtlogging.html#qWarning" translate="no">qWarning</a>() for details; this is not specific to autotests. If warnings would otherwise be lost in vast test logs, the occasional run with this environment variable set can help you to find and eliminate any that do arise.</p>
<h3 id="avoid-printing-debug-messages-from-autotests">Avoid Printing Debug Messages from Autotests</h3>
<p>Autotests should not produce any unhandled warning or debug messages. This will allow the CI Gate to treat new warning or debug messages as test failures.</p>
<p>Adding debug messages during development is fine, but these should be either disabled or removed before a test is checked in.</p>
<h3 id="write-well-structured-diagnostic-code">Write Well-structured Diagnostic Code</h3>
<p>Any diagnostic output that would be useful if a test fails should be part of the regular test output rather than being commented-out, disabled by preprocessor directives, or enabled only in debug builds. If a test fails during continuous integration, having all of the relevant diagnostic output in the CI logs could save you a lot of time compared to enabling the diagnostic code and testing again. Epecially, if the failure was on a platform that you don't have on your desktop.</p>
<p>Diagnostic messages in tests should use Qt's output mechanisms, such as <code translate="no">qDebug()</code> and <code translate="no">qWarning()</code>, rather than <code translate="no">stdio.h</code> or <code translate="no">iostream.h</code> output mechanisms. The latter bypass Qt's message handling and prevent the <code translate="no">-silent</code> command-line option from suppressing the diagnostic messages. This could result in important failure messages being hidden in a large volume of debugging output.</p>
<h2 id="writing-testable-code">Writing Testable Code</h2>
<p>The following sections provide guidelines for writing code that is easy to test:</p>
<ul>
<li><a href="qttest-best-practices-qdoc.html#break-dependencies" translate="no">Break Dependencies</a></li>
<li><a href="qttest-best-practices-qdoc.html#compile-all-classes-into-libraries" translate="no">Compile All Classes into Libraries</a></li>
</ul>
<h3 id="break-dependencies">Break Dependencies</h3>
<p>The idea of unit testing is to use every class in isolation. Since many classes instantiate other classes, it is not possible to instantiate one class separately. Therefore, you should use a technique called <i>dependency injection</i> that separates object creation from object use. A factory is responsible for building object trees. Other objects manipulate these objects through abstract interfaces.</p>
<p>This technique works well for data-driven applications. For GUI applications, this approach can be difficult as objects are frequently created and destructed. To verify the correct behavior of classes that depend on abstract interfaces, <i>mocking</i> can be used. For example, see <a href="https://github.com/google/googletest/tree/master/googlemock" translate="no">Googletest Mocking (gMock) Framework</a>.</p>
<h3 id="compile-all-classes-into-libraries">Compile All Classes into Libraries</h3>
<p>In small to medium sized projects, a build script typically lists all source files and then compiles the executable in one go. This means that the build scripts for the tests must list the needed source files again.</p>
<p>It is easier to list the source files and the headers only once in a script to build a static library. Then the <code translate="no">main()</code> function will be linked against the static library to build the executable and the tests will be linked against the static libraries.</p>
<p>For projects where the same source files are used in building several programs, it may be more appropriate to build the shared classes into a dynamically-linked (or shared object) library that each program, including the test programs, can load at run-time. Again, having the compiled code in a library helps to avoid duplication in the description of which components to combine to make the various programs.</p>
<h2 id="setting-up-test-machines">Setting up Test Machines</h2>
<p>The following sections discuss common problems caused by test machine setup:</p>
<ul>
<li><a href="qttest-best-practices-qdoc.html#screen-savers" translate="no">Screen Savers</a></li>
<li><a href="qttest-best-practices-qdoc.html#system-dialogs" translate="no">System Dialogs</a></li>
<li><a href="qttest-best-practices-qdoc.html#display-usage" translate="no">Display Usage</a></li>
<li><a href="qttest-best-practices-qdoc.html#window-managers" translate="no">Window Managers</a></li>
</ul>
<p>All of these problems can typically be solved by the judicious use of virtualisation.</p>
<h3 id="screen-savers">Screen Savers</h3>
<p>Screen savers can interfere with some of the tests for GUI classes, causing unreliable test results. Screen savers should be disabled to ensure that test results are consistent and reliable.</p>
<h3 id="system-dialogs">System Dialogs</h3>
<p>Dialogs displayed unexpectedly by the operating system or other running applications can steal input focus from widgets involved in an autotest, causing unreproducible failures.</p>
<p>Examples of typical problems include online update notification dialogs on macOS, false alarms from virus scanners, scheduled tasks such as virus signature updates, software updates pushed out to workstations, and chat programs popping up windows on top of the stack.</p>
<h3 id="display-usage">Display Usage</h3>
<p>Some tests use the test machine's display, mouse, and keyboard, and can thus fail if the machine is being used for something else at the same time or if multiple tests are run in parallel.</p>
<p>The CI system uses dedicated test machines to avoid this problem, but if you don't have a dedicated test machine, you may be able to solve this problem by running the tests on a second display.</p>
<p>On Unix, one can also run the tests on a nested or virtual X-server, such as Xephyr. For example, to run the entire set of tests on Xephyr, execute the following commands:</p>
<pre class="cpp" translate="no">
 Xephyr :<span class="number">1</span> <span class="operator">-</span>ac <span class="operator">-</span>screen <span class="number">1920x1200</span> <span class="operator">&gt;</span><span class="operator">/</span>dev<span class="operator">/</span>null <span class="number">2</span><span class="operator">&gt;</span><span class="operator">&amp;</span><span class="number">1</span> <span class="operator">&amp;</span>
 sleep <span class="number">5</span>
 DISPLAY<span class="operator">=</span>:<span class="number">1</span> icewm <span class="operator">&gt;</span><span class="operator">/</span>dev<span class="operator">/</span>null <span class="number">2</span><span class="operator">&gt;</span><span class="operator">&amp;</span><span class="number">1</span> <span class="operator">&amp;</span>
 cd tests<span class="operator">/</span><span class="keyword">auto</span>
 make
 DISPLAY<span class="operator">=</span>:<span class="number">1</span> make <span class="operator">-</span>k <span class="operator">-</span>j1 check
</pre>
<p>Users of NVIDIA binary drivers should note that Xephyr might not be able to provide GLX extensions. Forcing Mesa libGL might help:</p>
<pre class="cpp" translate="no">
 <span class="keyword">export</span> LD_PRELOAD<span class="operator">=</span><span class="operator">/</span>usr<span class="operator">/</span>lib<span class="operator">/</span>mesa<span class="operator">-</span>diverted<span class="operator">/</span>x86_64<span class="operator">-</span>linux<span class="operator">-</span>gnu<span class="operator">/</span>libGL<span class="operator">.</span>so<span class="operator">.</span><span class="number">1</span>
</pre>
<p>However, when tests are run on Xephyr and the real X-server with different libGL versions, the QML disk cache can make the tests crash. To avoid this, use <code translate="no">QML_DISABLE_DISK_CACHE=1</code>.</p>
<p>Alternatively, use the offscreen plugin:</p>
<pre class="cpp" translate="no">
 TESTARGS<span class="operator">=</span><span class="string">&quot;-platform offscreen&quot;</span> make check <span class="operator">-</span>k <span class="operator">-</span>j1
</pre>
<h3 id="window-managers">Window Managers</h3>
<p>On Unix, at least two autotests (<code translate="no">tst_examples</code> and <code translate="no">tst_gestures</code>) require a window manager to be running. Therefore, if running these tests under a nested X-server, you must also run a window manager in that X-server.</p>
<p>Your window manager must be configured to position all windows on the display automatically. Some windows managers, such as Tab Window Manager (twm), have a mode for manually positioning new windows, and this prevents the test suite from running without user interaction.</p>
<div class="admonition note">
<p><b>Note: </b>Tab Window Manager is not suitable for running the full suite of Qt autotests, as the <code translate="no">tst_gestures</code> autotest causes it to forget its configuration and revert to manual window placement.</p>
</div>
</div>
<!-- @@@qttest-best-practices.qdoc -->
        </div>
       </div>
   </div>
   </div>
</div>
<div class="footer">
   <p>
   <acronym title="Copyright">&copy;</acronym> 2024 <span translate="no">The Qt Company Ltd.</span>
   Documentation contributions included herein are the copyrights of
   their respective owners.<br/>    The documentation provided herein is licensed under the terms of the    <a href="http://www.gnu.org/licenses/fdl.html">GNU Free Documentation    License version 1.3</a> as published by the <span translate="no">Free Software Foundation</span>.<br/>    <span translate="no">Qt</span> and respective logos are <a href="https://doc.qt.io/qt/trademarks.html">    trademarks</a> of <span translate="no">The Qt Company Ltd.</span> in Finland and/or other countries
   worldwide. All other trademarks are property of their respective owners. </p>
</div>
</body>
</html>
